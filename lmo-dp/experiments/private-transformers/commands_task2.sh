
CUDA_VISIBLE_DEVICES=0 python3 -m table2text.run_language_modeling_lmo --output_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_0.3 --overwrite_output_dir --task_mode e2e --model_name_or_path $GPT-2 --tokenizer_name GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_0.3 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 16.302155663591545 --lmo_filepath experiments/lmo_noise_parameters/table2/lmo_epsLMO.json --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/LMO.E2E.GPT-2.eps_0.3 2>&1 > ../../running_logs/table2.LMO.E2E.GPT-2.eps_0.3.log &


CUDA_VISIBLE_DEVICES=1 python3 -m table2text.run_language_modeling_lmo --output_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_0.7 --overwrite_output_dir --task_mode e2e --model_name_or_path $GPT-2 --tokenizer_name GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_0.7 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 6.986638141539235 --lmo_filepath experiments/lmo_noise_parameters/table2/lmo_epsLMO.json --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/LMO.E2E.GPT-2.eps_0.7 2>&1 > ../../running_logs/table2.LMO.E2E.GPT-2.eps_0.7.log &


CUDA_VISIBLE_DEVICES=2 python3 -m table2text.run_language_modeling_lmo --output_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_2 --overwrite_output_dir --task_mode e2e --model_name_or_path $GPT-2 --tokenizer_name GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_2 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 2.445323349538732 --lmo_filepath experiments/lmo_noise_parameters/table2/lmo_epsLMO.json --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/LMO.E2E.GPT-2.eps_2 2>&1 > ../../running_logs/table2.LMO.E2E.GPT-2.eps_2.log &


CUDA_VISIBLE_DEVICES=3 python3 -m table2text.run_language_modeling_lmo --output_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_3 --overwrite_output_dir --task_mode e2e --model_name_or_path $GPT-2 --tokenizer_name GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/LMO.E2E.GPT-2.eps_3 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 1.6302155663591547 --lmo_filepath experiments/lmo_noise_parameters/table2/lmo_epsLMO.json --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/LMO.E2E.GPT-2.eps_3 2>&1 > ../../running_logs/table2.LMO.E2E.GPT-2.eps_3.log &


CUDA_VISIBLE_DEVICES=4 python3 -m table2text.run_language_modeling --output_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_0.3 --overwrite_output_dir --task_mode e2e --model_name_or_path GPT-2 --tokenizer_name $GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_0.3 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 16.302155663591545 --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/Gaussian.E2E.GPT-2.eps_0.3 2>&1 > ../../running_logs/table2.Gaussian.E2E.GPT-2.eps_0.3.log &


CUDA_VISIBLE_DEVICES=5 python3 -m table2text.run_language_modeling --output_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_0.7 --overwrite_output_dir --task_mode e2e --model_name_or_path GPT-2 --tokenizer_name $GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_0.7 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 6.986638141539235 --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/Gaussian.E2E.GPT-2.eps_0.7 2>&1 > ../../running_logs/table2.Gaussian.E2E.GPT-2.eps_0.7.log &


CUDA_VISIBLE_DEVICES=6 python3 -m table2text.run_language_modeling --output_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_2 --overwrite_output_dir --task_mode e2e --model_name_or_path GPT-2 --tokenizer_name $GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_2 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 2.445323349538732 --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/Gaussian.E2E.GPT-2.eps_2 2>&1 > ../../running_logs/table2.Gaussian.E2E.GPT-2.eps_2.log &


CUDA_VISIBLE_DEVICES=7 python3 -m table2text.run_language_modeling --output_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_3 --overwrite_output_dir --task_mode e2e --model_name_or_path GPT-2 --tokenizer_name $GPT-2 --do_train --do_eval --line_by_line --save_steps 100 --save_total_limit 1 --save_at_last no --logging_dir ../results/table2/table2/Gaussian.E2E.GPT-2.eps_3 --logging_steps -1 --seed 0 --eval_steps 100 --eval_epochs 2 --max_eval_batches 100 --evaluation_strategy epoch --evaluate_before_training "no" --evaluate_during_training "yes" --per_device_eval_batch_size 10 --max_generations 9223372036854775807 --max_generations_train 10 --max_generations_valid 9223372036854775807 --max_train_examples 9223372036854775807 --max_valid_examples 9223372036854775807 --max_eval_examples 9223372036854775807 --data_folder /mnt/data/nlp/prefix-tuning --max_seq_len 100 --format_mode cat --per_example_max_grad_norm 1 --target_delta 8e-6 --noise_multiplier 1.6302155663591547 --learning_rate 2e-3 --lr_decay "no" --num_train_epochs 10 --per_device_train_batch_size 16 --gradient_accumulation_steps 64 --non_private no --clipping_mode ghost --cache_dir ../results/table2/table2/cache/Gaussian.E2E.GPT-2.eps_3 2>&1 > ../../running_logs/table2.Gaussian.E2E.GPT-2.eps_3.log &

